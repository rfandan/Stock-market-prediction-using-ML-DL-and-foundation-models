{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "intro",
            "metadata": {},
            "source": [
                "# Stage 1: Baselines & Classical ML\n",
                "\n",
                "Welcome to the first stage of the project. In this notebook, we establish the **\"Hard-to-Beat\"** benchmarks for our stock market prediction task. \n",
                "\n",
                "### **Objective**\n",
                "Predict the **Next-Day Log Return** of a stock using historical price and volume data. We will compare simple statistical baselines against classical Machine Learning models.\n",
                "\n",
                "### **Models Evaluated**\n",
                "1. **Persistence**: The \"No-Change\" model. Assumes tomorrow's return will be exactly the same as today's.\n",
                "2. **Rolling Mean**: Assumes the return will revert to its 20-day moving average.\n",
                "3. **Linear Regression**: A baseline ML model looking for linear relationships between features (lags, vol, etc.).\n",
                "4. **Histogram-based Gradient Boosting (HistGB)**: A non-linear ensemble model that can capture complex patterns.\n",
                "5. **ARIMA**: A classic time-series econometric model.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "setup",
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from tqdm import tqdm\n",
                "from sklearn.linear_model import LinearRegression\n",
                "from sklearn.ensemble import HistGradientBoostingRegressor\n",
                "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
                "from statsmodels.tsa.arima.model import ARIMA\n",
                "import os\n",
                "import warnings\n",
                "\n",
                "# Settings\n",
                "warnings.filterwarnings(\"ignore\")\n",
                "plt.style.use(\"seaborn-v0_8-muted\")\n",
                "sns.set_theme(style=\"whitegrid\")\n",
                "\n",
                "# Constants\n",
                "DATA_PATH = \"../data/processed/stock_data_processed.parquet\"\n",
                "RESULTS_DIR = \"../results\"\n",
                "os.makedirs(RESULTS_DIR, exist_ok=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "loading",
            "metadata": {},
            "source": [
                "## 1. Load the Master Feature Store\n",
                "We load the data prepared by `data_download_colab.ipynb`. Note how we use `float32` to keep the memory footprint small while maintaining precision."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "load_code",
            "metadata": {},
            "outputs": [],
            "source": [
                "if not os.path.exists(DATA_PATH):\n",
                "    raise FileNotFoundError(\"Please run data_download_colab.ipynb first to generate the parquet file.\")\n",
                "\n",
                "panel = pd.read_parquet(DATA_PATH)\n",
                "print(f\"✓ Loaded panel data: {panel.shape[0]} rows, {panel.shape[1]} columns\")\n",
                "print(f\"✓ Assets: {panel.index.get_level_values('ticker').unique().tolist()}\")\n",
                "panel.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "helpers_md",
            "metadata": {},
            "source": [
                "## 2. Configuration & Helpers\n",
                "We define our cross-validation strategy (Simple train/test split for time series) and our evaluation metrics. \n",
                "\n",
                "> **Why Directional Accuracy?** In trading, getting the *sign* (Up or Down) correct is often more profitable than getting the exact magnitude correct (MAE/RMSE)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "helpers_code",
            "metadata": {},
            "outputs": [],
            "source": [
                "TRAIN_RATIO = 0.8\n",
                "# Features precisely matching our 'Block A' engineered columns in the master script\n",
                "FEATURE_COLS = [\n",
                "    \"ret_lag1\", \"ret_lag2\", \"ret_lag5\",\n",
                "    \"roll_vol\", \"range_norm\", \"vol_zscore\", \"mkt_return\"\n",
                "]\n",
                "\n",
                "def compute_metrics(y_true, y_pred):\n",
                "    \"\"\"Calculates MAE, RMSE, and the percentage of days the direction was predicted correctly.\"\"\"\n",
                "    yt = np.asarray(y_true, dtype=float)\n",
                "    yp = np.asarray(y_pred, dtype=float)\n",
                "    \n",
                "    mae  = mean_absolute_error(yt, yp)\n",
                "    rmse = np.sqrt(mean_squared_error(yt, yp))\n",
                "    \n",
                "    # Directional Accuracy (Excluding 0 returns)\n",
                "    mask = yt != 0\n",
                "    dir_acc = np.mean(np.sign(yt[mask]) == np.sign(yp[mask])) if mask.any() else 0\n",
                "    \n",
                "    return mae, rmse, dir_acc\n",
                "\n",
                "def fit_best_arima(train_data, max_p=2, max_q=2):\n",
                "    \"\"\"Quick grid search to find the best ARIMA (p,0,q) based on AIC.\"\"\"\n",
                "    best_aic, best_fit = np.inf, None\n",
                "    for p in range(max_p + 1):\n",
                "        for q in range(max_q + 1):\n",
                "            if p == 0 and q == 0: continue\n",
                "            try:\n",
                "                model = ARIMA(train_data, order=(p, 0, q)).fit()\n",
                "                if model.aic < best_aic:\n",
                "                    best_aic, best_fit = model.aic, model\n",
                "            except: continue\n",
                "    return best_fit"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "eval_md",
            "metadata": {},
            "source": [
                "## 3. Benchmarking Pipeline\n",
                "We loop through each ticker (excluding SPY, as SPY is our market feature) and train our models."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "eval_code",
            "metadata": {},
            "outputs": [],
            "source": [
                "tickers = [t for t in panel.index.get_level_values(\"ticker\").unique() if t != \"SPY\"]\n",
                "all_results = []\n",
                "\n",
                "for ticker in tqdm(tickers, desc=\"Evaluating Assets\"):\n",
                "    # Extract ticker data\n",
                "    asset_df = panel.xs(ticker, level=\"ticker\")\n",
                "    \n",
                "    # Split features and target\n",
                "    X = asset_df[FEATURE_COLS]\n",
                "    y = asset_df[\"log_return\"]\n",
                "    \n",
                "    # Chronological Split (Train/Test)\n",
                "    split_idx = int(len(X) * TRAIN_RATIO)\n",
                "    X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
                "    y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
                "    \n",
                "    # 1. Persistence Baseline\n",
                "    # Uses 'ret_lag1' as the prediction for today's log_return\n",
                "    p_pred = X_test[\"ret_lag1\"]\n",
                "    \n",
                "    # 2. Linear Regression\n",
                "    lr = LinearRegression().fit(X_train, y_train)\n",
                "    lr_pred = lr.predict(X_test)\n",
                "    \n",
                "    # 3. HistGradientBoosting\n",
                "    hgb = HistGradientBoostingRegressor(random_state=42, max_iter=100)\n",
                "    hgb.fit(X_train, y_train)\n",
                "    hgb_pred = hgb.predict(X_test)\n",
                "    \n",
                "    # 4. ARIMA (p,0,q)\n",
                "    arima_model = fit_best_arima(y_train)\n",
                "    # If ARIMA fails, we use a simple zero-forecast\n",
                "    arima_pred = arima_model.forecast(steps=len(y_test)) if arima_model else np.zeros(len(y_test))\n",
                "    \n",
                "    # Aggregate Model Predictions\n",
                "    preds = {\n",
                "        \"Persistence\": p_pred,\n",
                "        \"LinearReg\": lr_pred,\n",
                "        \"HistGB\": hgb_pred,\n",
                "        \"ARIMA\": arima_pred\n",
                "    }\n",
                "    \n",
                "    # Compute Metrics for each model\n",
                "    for name, yp in preds.items():\n",
                "        mae, rmse, dacc = compute_metrics(y_test, yp)\n",
                "        all_results.append({\n",
                "            \"Ticker\": ticker,\n",
                "            \"Model\": name,\n",
                "            \"MAE\": mae,\n",
                "            \"RMSE\": rmse,\n",
                "            \"DirAcc\": dacc\n",
                "        })\n",
                "\n",
                "results_df = pd.DataFrame(all_results)\n",
                "results_df.to_csv(f\"{RESULTS_DIR}/stage1_results.csv\", index=False)\n",
                "print(f\"\\n✓ Results saved to {RESULTS_DIR}/stage1_results.csv\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "results_md",
            "metadata": {},
            "source": [
                "## 4. Performance Visualisation\n",
                "We compare the average Root Mean Squared Error (RMSE) and Directional Accuracy across all tickers."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "results_code",
            "metadata": {},
            "outputs": [],
            "source": [
                "summary = results_df.groupby(\"Model\")[[\"MAE\", \"RMSE\", \"DirAcc\"]].mean().sort_values(\"RMSE\")\n",
                "print(\"Average Performance Across all 12 identifiers:\")\n",
                "display(summary.style.highlight_min(subset=[\"MAE\", \"RMSE\"], color='lightgreen').highlight_max(subset=[\"DirAcc\"], color='lightgreen'))\n",
                "\n",
                "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
                "\n",
                "sns.barplot(data=results_df, x=\"Model\", y=\"RMSE\", ax=ax1, palette=\"viridis\")\n",
                "ax1.set_title(\"Root Mean Squared Error (Lower is Better)\")\n",
                "ax1.set_ylim(results_df[\"RMSE\"].min() * 0.9, results_df[\"RMSE\"].max() * 1.1)\n",
                "\n",
                "sns.barplot(data=results_df, x=\"Model\", y=\"DirAcc\", ax=ax2, palette=\"magma\")\n",
                "ax2.set_title(\"Directional Accuracy (Higher is Better)\")\n",
                "ax2.axhline(0.5, color='red', linestyle='--', label=\"Random Guess (50%)\")\n",
                "ax2.legend()\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
