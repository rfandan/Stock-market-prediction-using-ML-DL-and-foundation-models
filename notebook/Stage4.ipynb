{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "intro",
            "metadata": {},
            "source": [
                "# Stage 4: Probabilistic Forecasting (Quantile Regression)\n",
                "\n",
                "In the previous stages, we performed **Point Forecasting**—predicting exactly one number for tomorrow's return. However, financial markets are famously uncertain. \n",
                "\n",
                "### **What is Probabilistic Forecasting?**\n",
                "Instead of predicting just the *mean*, we predict the **distribution**. We do this using **Quantile Regression**. \n",
                "\n",
                "We will predict 3 key quantiles:\n",
                "1. **5th Percentile (q05)**: The \"worst case\" scenario (used for Value-at-Risk).\n",
                "2. **50th Percentile (q50)**: The median prediction (should be similar to Stage 2/3 results).\n",
                "3. **95th Percentile (q95)**: The \"best case\" scenario.\n",
                "\n",
                "### **The Pinball Loss Function**\n",
                "To train a model to predict specific quantiles, we use the **Pinball Loss** (also known as Quantile Loss). It penalizes the model differently based on whether it over-predicts or under-predicts, forcing the model to learn the boundaries of the data distribution.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "setup",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import warnings\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "from torch.utils.data import TensorDataset, DataLoader\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
                "from tqdm import tqdm\n",
                "\n",
                "# Settings\n",
                "warnings.filterwarnings(\"ignore\")\n",
                "sns.set_theme(style=\"whitegrid\")\n",
                "\n",
                "# Constants\n",
                "DATA_PATH = \"../data/processed/stock_data_processed.parquet\"\n",
                "RESULTS_DIR = \"../results\"\n",
                "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
                "\n",
                "# Hyperparameters\n",
                "LOOKBACK    = 20\n",
                "TRAIN_FRAC  = 0.70\n",
                "VAL_FRAC    = 0.10\n",
                "HIDDEN_DIM  = 64\n",
                "BATCH_SIZE  = 64\n",
                "EPOCHS      = 100\n",
                "PATIENCE    = 10\n",
                "LR          = 1e-4\n",
                "SEED        = 42\n",
                "\n",
                "# Quantiles to predict\n",
                "QUANTILES = [0.05, 0.50, 0.95]\n",
                "\n",
                "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"✓ Using Device: {DEVICE}\")\n",
                "\n",
                "np.random.seed(SEED)\n",
                "torch.manual_seed(SEED)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "models_md",
            "metadata": {},
            "source": [
                "## 1. Probabilistic Architectures\n",
                "We adapt the LSTM and Transformer architectures to output a vector of size `len(QUANTILES)` instead of a single scalar."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "models_code",
            "metadata": {},
            "outputs": [],
            "source": [
                "class LSTMQuantile(nn.Module):\n",
                "    def __init__(self, input_dim, hidden_dim, num_quantiles):\n",
                "        super().__init__()\n",
                "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=2, batch_first=True, dropout=0.2)\n",
                "        self.head = nn.Linear(hidden_dim, num_quantiles)\n",
                "\n",
                "    def forward(self, x):\n",
                "        out, _ = self.lstm(x)\n",
                "        return self.head(out[:, -1, :]) # (Batch, NumQuantiles)\n",
                "\n",
                "class TransformerQuantile(nn.Module):\n",
                "    def __init__(self, input_dim, embed_dim, num_quantiles):\n",
                "        super().__init__()\n",
                "        self.encoder = nn.Linear(input_dim, embed_dim)\n",
                "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=4, batch_first=True, dropout=0.1)\n",
                "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
                "        self.head = nn.Linear(embed_dim, num_quantiles)\n",
                "\n",
                "    def forward(self, x):\n",
                "        x = self.encoder(x)\n",
                "        x = self.transformer(x)\n",
                "        return self.head(x.mean(dim=1)) # Global Average Pooling"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "loss_md",
            "metadata": {},
            "source": [
                "## 2. Pinball Loss Implementation\n",
                "The core of quantile regression. If $y < \\hat{y}$ (over-prediction), we penalize by $(1-q)$. If $y > \\hat{y}$ (under-prediction), we penalize by $q$."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "loss_code",
            "metadata": {},
            "outputs": [],
            "source": [
                "def pinball_loss(y_pred, y_true, quantiles):\n",
                "    \"\"\"\n",
                "    y_pred: (Batch, NumQuantiles)\n",
                "    y_true: (Batch,)\n",
                "    \"\"\"\n",
                "    losses = []\n",
                "    for i, q in enumerate(quantiles):\n",
                "        error = y_true - y_pred[:, i]\n",
                "        loss = torch.max(q * error, (q - 1) * error)\n",
                "        losses.append(loss.mean())\n",
                "    return torch.stack(losses).mean()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "pipeline_md",
            "metadata": {},
            "source": [
                "## 3. Data & Training Pipeline\n",
                "We will run both models (LSTM and Transformer) on the **Engineered Features** (Stage 1 Feats) as they have historically performed best."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "pipeline_code",
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_sequences(features, target, lookback):\n",
                "    X, y = [], []\n",
                "    for i in range(len(features) - lookback):\n",
                "        X.append(features[i : i + lookback])\n",
                "        y.append(target[i + lookback])\n",
                "    return np.array(X), np.array(y)\n",
                "\n",
                "def train_quantile_model(model, train_dl, val_dl, epochs, lr, patience, device, quantiles):\n",
                "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
                "    best_val_loss = float('inf')\n",
                "    wait = 0\n",
                "    best_state = None\n",
                "    \n",
                "    for epoch in range(epochs):\n",
                "        model.train()\n",
                "        for xb, yb in train_dl:\n",
                "            xb, yb = xb.to(device), yb.to(device)\n",
                "            optimizer.zero_grad()\n",
                "            loss = pinball_loss(model(xb), yb, quantiles)\n",
                "            loss.backward()\n",
                "            optimizer.step()\n",
                "            \n",
                "        model.eval()\n",
                "        v_loss = 0\n",
                "        with torch.no_grad():\n",
                "            for xb, yb in val_dl:\n",
                "                xb, yb = xb.to(device), yb.to(device)\n",
                "                v_loss += pinball_loss(model(xb), yb, quantiles).item() * len(xb)\n",
                "        v_loss /= len(val_dl.dataset)\n",
                "        \n",
                "        if v_loss < best_val_loss:\n",
                "            best_val_loss = v_loss\n",
                "            best_state = model.state_dict()\n",
                "            wait = 0\n",
                "        else:\n",
                "            wait += 1\n",
                "            if wait >= patience: break\n",
                "            \n",
                "    model.load_state_dict(best_state)\n",
                "    return model\n",
                "\n",
                "panel = pd.read_parquet(DATA_PATH)\n",
                "tickers = [t for t in panel.index.get_level_values(\"ticker\").unique() if t != \"SPY\"]\n",
                "feat_cols = [\"log_return\", \"roll_vol\", \"range_norm\", \"vol_zscore\", \"mkt_return\"]\n",
                "\n",
                "stage4_results = []\n",
                "all_preds = {} # For plotting\n",
                "\n",
                "MODELS = {\n",
                "    \"LSTM-Prob\": lambda: LSTMQuantile(len(feat_cols), HIDDEN_DIM, len(QUANTILES)),\n",
                "    \"Trans-Prob\": lambda: TransformerQuantile(len(feat_cols), HIDDEN_DIM, len(QUANTILES))\n",
                "}\n",
                "\n",
                "for name, model_fn in MODELS.items():\n",
                "    print(f\"\\nEvaluating: {name}\")\n",
                "    for ticker in tqdm(tickers):\n",
                "        tk_data = panel.xs(ticker, level=\"ticker\")\n",
                "        feats = tk_data[feat_cols].values\n",
                "        target = tk_data[\"log_return\"].values\n",
                "        \n",
                "        # Sequences & Split\n",
                "        X_all, y_all = create_sequences(feats, target, LOOKBACK)\n",
                "        split1 = int(len(X_all) * TRAIN_FRAC)\n",
                "        split2 = int(len(X_all) * (TRAIN_FRAC + VAL_FRAC))\n",
                "        \n",
                "        X_tr, y_tr = X_all[:split1], y_all[:split1]\n",
                "        X_va, y_va = X_all[split1:split2], y_all[split1:split2]\n",
                "        X_te, y_te = X_all[split2:], y_all[split2:]\n",
                "        \n",
                "        # Scale\n",
                "        scaler = StandardScaler()\n",
                "        scaler.fit(X_tr.reshape(-1, len(feat_cols)))\n",
                "        X_tr = scaler.transform(X_tr.reshape(-1, len(feat_cols))).reshape(X_tr.shape)\n",
                "        X_va = scaler.transform(X_va.reshape(-1, len(feat_cols))).reshape(X_va.shape)\n",
                "        X_te = scaler.transform(X_te.reshape(-1, len(feat_cols))).reshape(X_te.shape)\n",
                "        \n",
                "        tr_dl = DataLoader(TensorDataset(torch.FloatTensor(X_tr), torch.FloatTensor(y_tr)), batch_size=BATCH_SIZE, shuffle=True)\n",
                "        va_dl = DataLoader(TensorDataset(torch.FloatTensor(X_va), torch.FloatTensor(y_va)), batch_size=BATCH_SIZE, shuffle=False)\n",
                "        \n",
                "        model = model_fn().to(DEVICE)\n",
                "        model = train_quantile_model(model, tr_dl, va_dl, EPOCHS, LR, PATIENCE, DEVICE, QUANTILES)\n",
                "        \n",
                "        # Predict\n",
                "        model.eval()\n",
                "        with torch.no_grad():\n",
                "            y_pred = model(torch.FloatTensor(X_te).to(DEVICE)).cpu().numpy()\n",
                "            \n",
                "        # Metrics (using Median q50 for MAE/RMSE)\n",
                "        mae = mean_absolute_error(y_te, y_pred[:, 1])\n",
                "        rmse = np.sqrt(mean_squared_error(y_te, y_pred[:, 1]))\n",
                "        \n",
                "        # Coverage: How many actual values fell inside the 90% interval (q05 to q95)?\n",
                "        coverage = np.mean((y_te >= y_pred[:, 0]) & (y_te <= y_pred[:, 2]))\n",
                "        \n",
                "        stage4_results.append({\"Ticker\": ticker, \"Model\": name, \"MAE\": mae, \"RMSE\": rmse, \"Coverage\": coverage})\n",
                "        all_preds[(name, ticker)] = (y_te, y_pred)\n",
                "\n",
                "res_df = pd.DataFrame(stage4_results)\n",
                "res_df.to_csv(f\"{RESULTS_DIR}/stage4_results.csv\", index=False)\n",
                "print(f\"\\n✓ Stage 4 Results saved.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "viz_md",
            "metadata": {},
            "source": [
                "## 4. Visualizing Uncertainty\n",
                "We plot the prediction intervals for a sample ticker. The shaded area represents the 90% confidence zone (q05 to q95)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "viz_code",
            "metadata": {},
            "outputs": [],
            "source": [
                "demo_ticker = \"BTC-USD\"\n",
                "plt.figure(figsize=(15, 7))\n",
                "\n",
                "y_true, y_pred = all_preds[(\"Trans-Prob\", demo_ticker)]\n",
                "last_n = 100\n",
                "plt.plot(y_true[-last_n:], label=\"Actual Return\", color=\"black\", alpha=0.6)\n",
                "plt.plot(y_pred[-last_n:, 1], label=\"Median Prediction (q50)\", color=\"blue\")\n",
                "plt.fill_between(range(last_n), y_pred[-last_n:, 0], y_pred[-last_n:, 2], \n",
                "                 color=\"blue\", alpha=0.2, label=\"90% Prediction Interval\")\n",
                "\n",
                "plt.title(f\"Stage 4: {demo_ticker} Probabilistic Forecast (Transformer)\")\n",
                "plt.legend()\n",
                "plt.show()\n",
                "\n",
                "avg_calib = res_df.groupby(\"Model\")[\"Coverage\"].mean()\n",
                "print(f\"Average 90% Interval Coverage:\\n{avg_calib}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "detailed_summary",
            "metadata": {},
            "source": [
                "# Stage 4: Probabilistic Forecasting — Detailed Summary\n",
                "\n",
                "## What This Stage Does\n",
                "\n",
                "In Stage 4, we moved beyond point estimates. Instead of asking \"What will the return be tomorrow?\", we asked **\"What is the range of possible returns tomorrow, and how much risk are we taking?\"**\n",
                "\n",
                "By using **Quantile Regression (Pinball Loss)**, we forced our models (LSTM and Transformer) to estimate the boundaries of the data distribution.\n",
                "\n",
                "---\n",
                "\n",
                "## Key Metrics Introduced\n",
                "\n",
                "| Metric | Definition | Significance |\n",
                "|---------|------------|--------------|\n",
                "| **Pinball Loss** | A loss function specialized for quantiles. | Unlike MSE, it allows us to target specific percentiles (e.g., 5th or 95th). |\n",
                "| **Coverage** | The percentage of actual values that fall within the predicted range. | For a 90% interval, we expect coverage to be exactly **0.90**. |\n",
                "| **Calibration** | How close the coverage is to the target quantile. | Tells us if our model is \"overconfident\" (coverage < 0.90) or \"underconfident\" (coverage > 0.90). |\n",
                "\n",
                "--- \n",
                "\n",
                "## Importance of the 5th Quantile (q05)\n",
                "In finance, the 5th percentile of returns is known as the **Value at Risk (VaR)**. It represents the \"worst-case\" loss you might expect 1 day out of 20. Predicting this accurately is more important for risk management than predicting the exact price movement.\n",
                "\n",
                "--- \n",
                "\n",
                "## What We Learned\n",
                "- **Uncertainty is Dynamic**: Notice how the shaded \"90% interval\" expands during volatile periods and shrinks during calm ones. Our models have learned to associate high volatility in the past with higher uncertainty in the future.\n",
                "- **Calibration is Hard**: Getting exactly 90% coverage on financial data is difficult because market distributions are \"fat-tailed\" (extreme events happen more often than a normal bell curve would predict).\n",
                "\n",
                "--- \n",
                "\n",
                "## Next Step: Foundation Models\n",
                "Now that we've built our own custom LSTMs and Transformers, it's time to see how we stack up against the \"giants.\" In **Stage 5**, we will use **Foundation Models for Time Series** (like Amazon's **Chronos**) to see if a model pre-trained on billions of data points can outperform our domain-specific models."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}