{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "intro",
            "metadata": {},
            "source": [
                "# Stage 2: LSTM Sequence Modeling — Ablation Study\n",
                "\n",
                "In this stage, we move from classical ML to **Deep Learning**. We build a sequence-to-scalar LSTM (Long Short-Term Memory) model to predict next-day returns.\n",
                "\n",
                "### **The Research Question**\n",
                "> **\"Does manual feature engineering help, or can a deep network learn everything from raw data?\"**\n",
                "\n",
                "We answer this through an **Ablation Study** comparing three variants of the same LSTM architecture:\n",
                "\n",
                "1. **LSTM-Raw**: Uses only the historical `log_return` stream (1 feature).\n",
                "2. **LSTM-OHLCV**: Uses raw intraday dynamics (Returns, Range, Volume Change - 6 features).\n",
                "3. **LSTM-Feat**: Uses the pre-engineered features from Stage 1 (Volatility, Market Return, Z-scores - 5 features).\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "setup",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import warnings\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "from torch.utils.data import TensorDataset, DataLoader\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
                "from tqdm import tqdm\n",
                "\n",
                "# Settings\n",
                "warnings.filterwarnings(\"ignore\")\n",
                "sns.set_theme(style=\"whitegrid\")\n",
                "\n",
                "# Constants\n",
                "DATA_PATH = \"../data/processed/stock_data_processed.parquet\"\n",
                "RESULTS_DIR = \"../results\"\n",
                "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
                "\n",
                "# Hyperparameters\n",
                "LOOKBACK    = 20\n",
                "TRAIN_FRAC  = 0.70\n",
                "VAL_FRAC    = 0.10\n",
                "HIDDEN_DIM  = 64\n",
                "NUM_LAYERS  = 2\n",
                "DROPOUT     = 0.2\n",
                "LR          = 1e-3\n",
                "BATCH_SIZE  = 64\n",
                "EPOCHS      = 100\n",
                "PATIENCE    = 10\n",
                "SEED        = 42\n",
                "\n",
                "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"✓ Using Device: {DEVICE}\")\n",
                "\n",
                "# Reproducibility\n",
                "np.random.seed(SEED)\n",
                "torch.manual_seed(SEED)\n",
                "if torch.cuda.is_available():\n",
                "    torch.cuda.manual_seed_all(SEED)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "variants_md",
            "metadata": {},
            "source": [
                "## 1. Define Ablation Variants\n",
                "We map the columns available in our **Master Feature Store** to the three experimental variants."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "variants_code",
            "metadata": {},
            "outputs": [],
            "source": [
                "VARIANTS = {\n",
                "    \"LSTM-Raw\": [\"log_return\"],\n",
                "    \"LSTM-OHLCV\": [\n",
                "        \"log_return\", \"oc_return\", \"hl_range\", \n",
                "        \"close_pos\", \"log_vol\", \"vol_change\"\n",
                "    ],\n",
                "    \"LSTM-Feat\": [\n",
                "        \"log_return\", \"roll_vol\", \"range_norm\", \n",
                "        \"vol_zscore\", \"mkt_return\"\n",
                "    ]\n",
                "}\n",
                "\n",
                "panel = pd.read_parquet(DATA_PATH)\n",
                "print(f\"✓ Loaded Master Panel: {panel.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "architecture_md",
            "metadata": {},
            "source": [
                "## 2. Model Architecture & Helpers\n",
                "The model consists of a standard LSTM encoder followed by a Multi-Layer Perceptron (MLP) head. Only the **last hidden state** of the sequence is used for prediction."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "architecture_code",
            "metadata": {},
            "outputs": [],
            "source": [
                "class LSTMForecaster(nn.Module):\n",
                "    def __init__(self, input_dim, hidden_dim, num_layers, dropout):\n",
                "        super().__init__()\n",
                "        self.lstm = nn.LSTM(\n",
                "            input_size=input_dim, \n",
                "            hidden_size=hidden_dim, \n",
                "            num_layers=num_layers, \n",
                "            batch_first=True, \n",
                "            dropout=dropout if num_layers > 1 else 0\n",
                "        )\n",
                "        self.head = nn.Sequential(\n",
                "            nn.Linear(hidden_dim, 32),\n",
                "            nn.ReLU(),\n",
                "            nn.Linear(32, 1)\n",
                "        )\n",
                "\n",
                "    def forward(self, x):\n",
                "        out, _ = self.lstm(x)             # out: (Batch, Seq, Hidden)\n",
                "        return self.head(out[:, -1, :]).squeeze(-1) # Take last step\n",
                "\n",
                "def create_sequences(features, target, lookback):\n",
                "    X, y = [], []\n",
                "    for i in range(len(features) - lookback):\n",
                "        X.append(features[i : i + lookback])\n",
                "        y.append(target[i + lookback])\n",
                "    return np.array(X), np.array(y)\n",
                "\n",
                "def compute_metrics(y_true, y_pred):\n",
                "    mae = mean_absolute_error(y_true, y_pred)\n",
                "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
                "    dir_acc = np.mean(np.sign(y_true) == np.sign(y_pred))\n",
                "    return mae, rmse, dir_acc"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "training_md",
            "metadata": {},
            "source": [
                "## 3. Training Logic\n",
                "We use Early Stopping to prevent overfitting and Adam optimizer for robust weight updates."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "training_code",
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_model(model, train_dl, val_dl, epochs, lr, patience, device):\n",
                "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
                "    criterion = nn.MSELoss()\n",
                "    best_val_loss = float('inf')\n",
                "    wait = 0\n",
                "    best_state = None\n",
                "    \n",
                "    for epoch in range(epochs):\n",
                "        model.train()\n",
                "        for xb, yb in train_dl:\n",
                "            xb, yb = xb.to(device), yb.to(device)\n",
                "            optimizer.zero_grad()\n",
                "            loss = criterion(model(xb), yb)\n",
                "            loss.backward()\n",
                "            optimizer.step()\n",
                "            \n",
                "        model.eval()\n",
                "        v_loss = 0\n",
                "        with torch.no_grad():\n",
                "            for xb, yb in val_dl:\n",
                "                xb, yb = xb.to(device), yb.to(device)\n",
                "                v_loss += criterion(model(xb), yb).item() * len(xb)\n",
                "        v_loss /= len(val_dl.dataset)\n",
                "        \n",
                "        if v_loss < best_val_loss:\n",
                "            best_val_loss = v_loss\n",
                "            best_state = model.state_dict()\n",
                "            wait = 0\n",
                "        else:\n",
                "            wait += 1\n",
                "            if wait >= patience: break\n",
                "            \n",
                "    model.load_state_dict(best_state)\n",
                "    return model"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "pipeline_md",
            "metadata": {},
            "source": [
                "## 4. Execution Pipeline\n",
                "We loop through each variant and each ticker, performing a fair head-to-head comparison."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "pipeline_code",
            "metadata": {},
            "outputs": [],
            "source": [
                "tickers = [t for t in panel.index.get_level_values(\"ticker\").unique() if t != \"SPY\"]\n",
                "stage2_results = []\n",
                "\n",
                "for var_name, feat_cols in VARIANTS.items():\n",
                "    print(f\"\\n{'='*40}\\nEvaluating Variant: {var_name}\\n{'='*40}\")\n",
                "    \n",
                "    for ticker in tqdm(tickers, desc=f\"{var_name} Training\"):\n",
                "        tk_data = panel.xs(ticker, level=\"ticker\")\n",
                "        feats = tk_data[feat_cols].values\n",
                "        target = tk_data[\"log_return\"].values\n",
                "        \n",
                "        # 1. Split\n",
                "        n = len(feats)\n",
                "        n_train = int(n * TRAIN_FRAC)\n",
                "        n_val   = int(n * (TRAIN_FRAC + VAL_FRAC))\n",
                "        \n",
                "        X_all, y_all = create_sequences(feats, target, LOOKBACK)\n",
                "        \n",
                "        n_seq = len(X_all)\n",
                "        split1 = int(n_seq * TRAIN_FRAC)\n",
                "        split2 = int(n_seq * (TRAIN_FRAC + VAL_FRAC))\n",
                "        \n",
                "        X_tr, y_tr = X_all[:split1], y_all[:split1]\n",
                "        X_va, y_va = X_all[split1:split2], y_all[split1:split2]\n",
                "        X_te, y_te = X_all[split2:], y_all[split2:]\n",
                "        \n",
                "        # 2. Scale (on Train ONLY to avoid leakage)\n",
                "        scaler = StandardScaler()\n",
                "        scaler.fit(X_tr.reshape(-1, len(feat_cols)))\n",
                "        \n",
                "        X_tr = scaler.transform(X_tr.reshape(-1, len(feat_cols))).reshape(X_tr.shape)\n",
                "        X_va = scaler.transform(X_va.reshape(-1, len(feat_cols))).reshape(X_va.shape)\n",
                "        X_te = scaler.transform(X_te.reshape(-1, len(feat_cols))).reshape(X_te.shape)\n",
                "        \n",
                "        # 3. Loaders\n",
                "        tr_dl = DataLoader(TensorDataset(torch.FloatTensor(X_tr), torch.FloatTensor(y_tr)), batch_size=BATCH_SIZE, shuffle=True)\n",
                "        va_dl = DataLoader(TensorDataset(torch.FloatTensor(X_va), torch.FloatTensor(y_va)), batch_size=BATCH_SIZE, shuffle=False)\n",
                "        \n",
                "        # 4. Model & Train\n",
                "        model = LSTMForecaster(len(feat_cols), HIDDEN_DIM, NUM_LAYERS, DROPOUT).to(DEVICE)\n",
                "        model = train_model(model, tr_dl, va_dl, EPOCHS, LR, PATIENCE, DEVICE)\n",
                "        \n",
                "        # 5. Evaluate\n",
                "        model.eval()\n",
                "        with torch.no_grad():\n",
                "            y_pred = model(torch.FloatTensor(X_te).to(DEVICE)).cpu().numpy()\n",
                "            \n",
                "        mae, rmse, dacc = compute_metrics(y_te, y_pred)\n",
                "        stage2_results.append({\"Ticker\": ticker, \"Model\": var_name, \"MAE\": mae, \"RMSE\": rmse, \"DirAcc\": dacc})\n",
                "\n",
                "res_df = pd.DataFrame(stage2_results)\n",
                "res_df.to_csv(f\"{RESULTS_DIR}/stage2_results.csv\", index=False)\n",
                "print(f\"\\n✓ Stage 2 Results saved to {RESULTS_DIR}/stage2_results.csv\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "viz_md",
            "metadata": {},
            "source": [
                "## 5. Visual Summary\n",
                "We compare the average accuracy and error rates across the three variants."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "viz_code",
            "metadata": {},
            "outputs": [],
            "source": [
                "summary = res_df.groupby(\"Model\")[[\"MAE\", \"RMSE\", \"DirAcc\"]].mean().sort_values(\"RMSE\")\n",
                "display(summary.style.background_gradient(cmap=\"Blues\"))\n",
                "\n",
                "plt.figure(figsize=(12, 6))\n",
                "sns.boxplot(data=res_df, x=\"Model\", y=\"DirAcc\", palette=\"Set2\")\n",
                "plt.axhline(0.5, color='red', linestyle='--')\n",
                "plt.title(\"Directional Accuracy Distribution by Variant\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e7281b47",
            "metadata": {},
            "source": [
                "# Stage 2: LSTM Sequence Modeling — Ablation Study\n",
                "\n",
                "## What This Stage Does\n",
                "\n",
                "Stage 2 builds on the baselines established in Stage 1 by introducing **deep learning** via LSTM (Long Short-Term Memory) networks for next-day return prediction. Instead of training a single LSTM, we run a structured **ablation study** — three LSTM variants that differ *only* in what information they receive as input, while keeping the architecture identical.\n",
                "\n",
                "This answers a fundamental question in applied ML:\n",
                "\n",
                "> **Does hand-crafted feature engineering help, or can a recurrent network learn everything it needs from raw data?**\n",
                "\n",
                "---\n",
                "\n",
                "## The Three Variants\n",
                "\n",
                "| Variant | Inputs per Time Step | What It Tests |\n",
                "|---------|---------------------|---------------|\n",
                "| **LSTM-Raw** | Log return only (1 feature) | Can the LSTM discover patterns like volatility clustering and momentum purely from a single return stream? |\n",
                "| **LSTM-OHLCV** | Log return + 5 minimally processed OHLCV features (6 features) | Does richer raw market data (intraday range, close position, volume dynamics) help without imposing assumptions like rolling windows? |\n",
                "| **LSTM-Feat** | Log return + 4 engineered features (5 features) | Does explicit domain-knowledge engineering (rolling volatility, volume z-score, market return) give an edge over raw data? |\n",
                "\n",
                "### OHLCV Features Explained\n",
                "\n",
                "These are lightweight transformations of raw daily bars — no rolling windows, no cross-asset information:\n",
                "\n",
                "| Feature | Formula | Intuition |\n",
                "|---------|---------|-----------|\n",
                "| `oc_return` | (close − open) / open | Intraday momentum — did price rise or fall during trading? |\n",
                "| `hl_range` | (high − low) / close | Intraday volatility — how wide was the day's range? |\n",
                "| `close_pos` | (close − low) / (high − low) | Where did price close within the day's range? (0 = at low, 1 = at high) |\n",
                "| `log_vol` | log(1 + volume) | Absolute trading activity level |\n",
                "| `vol_change` | log(volume_t / volume_{t−1}) | Change in trading activity — is interest rising or falling? |\n",
                "\n",
                "### Engineered Features (from Stage 1)\n",
                "\n",
                "| Feature | Description |\n",
                "|---------|-------------|\n",
                "| `roll_vol` | 20-day rolling standard deviation of log returns |\n",
                "| `range_norm` | Normalized high-low range |\n",
                "| `vol_zscore` | Volume z-score over 20-day window |\n",
                "| `mkt_return` | SPY log return (market factor) |\n",
                "\n",
                "---\n",
                "\n",
                "## How It Works\n",
                "\n",
                "### Pipeline Overview\n",
                "\n",
                "```\n",
                "Download 12 tickers (same universe as Stage 1)\n",
                "        │\n",
                "        ▼\n",
                "Compute all features (unshifted — windowing handles lag)\n",
                "        │\n",
                "        ▼\n",
                "For each variant × each ticker:\n",
                "        │\n",
                "        ├── Slice relevant feature columns\n",
                "        ├── Create lookback sequences: [t−20 … t−1] → predict return at t\n",
                "        ├── Split: 70% train / 10% val / 20% test\n",
                "        ├── Fit StandardScaler on train only\n",
                "        ├── Train LSTM with early stopping\n",
                "        ├── Predict on test set\n",
                "        └── Record MAE, RMSE, Directional Accuracy\n",
                "        │\n",
                "        ▼\n",
                "Compare all 3 variants (+ Stage 1 baselines if available)\n",
                "```\n",
                "\n",
                "### Avoiding Data Leakage\n",
                "\n",
                "A critical concern with time series + deep learning. Here's how each source of leakage is prevented:\n",
                "\n",
                "| Leakage Risk | Mitigation |\n",
                "|--------------|------------|\n",
                "| Future data in features | No features are shifted. Instead, the sequence window `[t−20 … t−1]` naturally uses only past data to predict day `t`. |\n",
                "| Future data in scaling | `StandardScaler` is fit **only on training data**, then applied to val/test via `transform()`. |\n",
                "| Future data in splits | Chronological split — no shuffling. Train comes before val comes before test. |\n",
                "| Cross-validation bleed | No cross-validation. Single forward split preserves temporal ordering. |\n",
                "\n",
                "### LSTM Architecture\n",
                "\n",
                "```\n",
                "Input (batch, 20, F)\n",
                "    │\n",
                "    ▼\n",
                "LSTM ── 2 layers, 64 hidden units, dropout 0.2 between layers\n",
                "    │\n",
                "    ▼\n",
                "Last hidden state (batch, 64)\n",
                "    │\n",
                "    ▼\n",
                "Linear(64, 32) → ReLU → Linear(32, 1)\n",
                "    │\n",
                "    ▼\n",
                "Scalar prediction: next-day log return\n",
                "```\n",
                "\n",
                "**Why this architecture?**\n",
                "\n",
                "- **2 layers, 64 units** — small enough to train on ~2,500 samples per ticker without severe overfitting, large enough to capture nonlinear temporal patterns.\n",
                "- **Last hidden state only** — the final time step's hidden state summarises the entire input sequence. Simpler and more stable than attention over all steps (that's Stage 3).\n",
                "- **MLP head** — a single linear layer would limit the model to linear mappings from hidden state to prediction. The ReLU layer adds one more nonlinear transformation.\n",
                "\n",
                "### Training Details\n",
                "\n",
                "| Setting | Value | Why |\n",
                "|---------|-------|-----|\n",
                "| Optimizer | Adam | Standard; adapts learning rates per parameter |\n",
                "| Learning rate | 1e-3 | Default starting point for Adam |\n",
                "| Loss | MSE | Standard for regression; directly optimises RMSE |\n",
                "| Batch size | 64 | Balances gradient noise and training speed |\n",
                "| Max epochs | 100 | Upper bound; rarely reached due to early stopping |\n",
                "| Early stopping patience | 10 | Stops training if validation loss doesn't improve for 10 consecutive epochs |\n",
                "| Gradient clipping | max_norm = 1.0 | Prevents exploding gradients, common with LSTMs |\n",
                "| Seed | 42 (reset per variant × ticker) | Ensures identical weight initialisation and data shuffling across variants for fair comparison |\n",
                "\n",
                "---\n",
                "\n",
                "## What to Expect\n",
                "\n",
                "### Realistic Expectations\n",
                "\n",
                "Daily stock returns are **extremely noisy**. The signal-to-noise ratio is very low. You should expect:\n",
                "\n",
                "- **MAE and RMSE** close to (but hopefully slightly below) the Stage 1 baselines\n",
                "- **Directional accuracy** in the range of **48–54%** — even 52% is meaningful\n",
                "- **LSTM-Raw will likely underperform** — 1 feature × 20 steps gives the network very little to work with given limited training data (~2,500 samples)\n",
                "- **LSTM-Feat will likely perform best** — pre-computed rolling statistics compensate for the small dataset by injecting domain knowledge\n",
                "- **LSTM-OHLCV is the interesting middle ground** — if it matches LSTM-Feat, it means the LSTM can extract useful patterns from raw market structure without hand-holding\n",
                "\n",
                "### Possible Outcomes and Interpretation\n",
                "\n",
                "| Outcome | What It Means |\n",
                "|---------|---------------|\n",
                "| LSTM-Feat > LSTM-OHLCV > LSTM-Raw | Feature engineering helps; the network can't efficiently rediscover rolling statistics from limited data |\n",
                "| LSTM-OHLCV ≈ LSTM-Feat > LSTM-Raw | The LSTM learns its own features from rich raw data, but needs more than just returns |\n",
                "| All three ≈ Stage 1 baselines | Daily returns are too noisy for LSTMs to add value over simple models (an honest and valid finding) |\n",
                "| LSTM-Raw > others | The engineered features were actually adding noise; pure signal is better (unlikely but possible) |\n",
                "\n",
                "**Any of these outcomes is a valid and interesting result.** The goal is not to \"win\" but to understand *when and why* complexity helps.\n",
                "\n",
                "---\n",
                "\n",
                "## Outputs\n",
                "\n",
                "### Files\n",
                "\n",
                "| File | Description |\n",
                "|------|-------------|\n",
                "| `stage2_results.csv` | MAE, RMSE, DirAcc for every (ticker, variant) pair |\n",
                "| `stage2_all_predictions.csv` | Actual vs predicted values for all test days |\n",
                "\n",
                "### Plots\n",
                "\n",
                "| Plot | What It Shows |\n",
                "|------|---------------|\n",
                "| `stage2_ablation_bars.png` | Mean MAE / RMSE / DirAcc across tickers for each variant |\n",
                "| `stage2_ablation_per_ticker.png` | Grouped bar chart — RMSE by ticker with all 3 variants side by side |\n",
                "| `stage2_ablation_curves.png` | Training vs validation loss over epochs (convergence and overfitting) |\n",
                "| `stage2_ablation_predictions.png` | Actual vs predicted returns for a demo ticker (all 3 variants overlaid) |\n",
                "| `stage2_ablation_heatmap.png` | Directional accuracy heatmap — ticker × variant |\n",
                "| `stage2_full_comparison.png` | Combined comparison with Stage 1 baselines (if `stage1_results.csv` exists) |\n",
                "\n",
                "---\n",
                "\n",
                "## Connection to Other Stages\n",
                "\n",
                "```\n",
                "Stage 1: Baselines ──────────── \"Can simple models predict returns?\"\n",
                "    │\n",
                "Stage 2: LSTM (you are here) ── \"Does sequential deep learning help?\n",
                "    │                             Does feature engineering matter?\"\n",
                "    │\n",
                "Stage 3: Transformer ────────── \"Do attention mechanisms over the\n",
                "    │                             sequence add value over LSTMs?\"\n",
                "    │\n",
                "Stage 4: Probabilistic ──────── \"Can we predict uncertainty,\n",
                "    │                             not just point estimates?\"\n",
                "    │\n",
                "Stage 5: Foundation Models ──── \"Can a pretrained model match\n",
                "    │                             custom-trained ones?\"\n",
                "    │\n",
                "Stage 6: Latent Space ───────── \"What structure exists in the\n",
                "    │                             learned representations?\"\n",
                "    │\n",
                "Stage 7: Final Comparison ───── \"What did we learn?\"\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "## How to Run\n",
                "\n",
                "```bash\n",
                "# Make sure Stage 1 has been run first (for comparison)\n",
                "python stage1_baselines.py\n",
                "\n",
                "# Run Stage 2\n",
                "python stage2_lstm_ablation.py\n",
                "```\n",
                "\n",
                "**Requirements:** Python 3.9+, PyTorch, yfinance, scikit-learn, pandas, numpy, matplotlib, tqdm\n",
                "\n",
                "**Runtime:** ~15–30 minutes on CPU (12 tickers × 3 variants), ~5 minutes on GPU.\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}