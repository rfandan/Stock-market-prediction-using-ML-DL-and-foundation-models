{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "intro",
            "metadata": {},
            "source": [
                "# Stage 3: Transformer with Attention — Ablation Study\n",
                "\n",
                "In this stage, we upgrade our deep learning architecture from Recurrent Neural Networks (LSTMs) to **Transformers**. \n",
                "\n",
                "### **Why Transformers?**\n",
                "Unlike LSTMs, which process data sequentially and can \"forget\" early parts of a sequence, Transformers use **Self-Attention** to look at the entire 20-day window simultaneously. They decide which specific days in the past hold the most \"signal\" for tomorrow's prediction.\n",
                "\n",
                "### **Ablation Study**\n",
                "We maintain the same three variants to ensure a fair comparison across all modeling stages:\n",
                "1. **Transformer-Raw**: Log returns only.\n",
                "2. **Transformer-OHLCV**: Raw intraday dynamics.\n",
                "3. **Transformer-Feat**: Engineered features (Volatility, Market, etc.).\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "setup",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import warnings\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "from torch.utils.data import TensorDataset, DataLoader\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
                "from tqdm import tqdm\n",
                "\n",
                "# Settings\n",
                "warnings.filterwarnings(\"ignore\")\n",
                "sns.set_theme(style=\"whitegrid\")\n",
                "\n",
                "# Constants\n",
                "DATA_PATH = \"../data/processed/stock_data_processed.parquet\"\n",
                "RESULTS_DIR = \"../results\"\n",
                "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
                "\n",
                "# Hyperparameters\n",
                "LOOKBACK    = 20\n",
                "TRAIN_FRAC  = 0.70\n",
                "VAL_FRAC    = 0.10\n",
                "EMBED_DIM   = 64\n",
                "NUM_HEADS   = 4\n",
                "NUM_LAYERS  = 2\n",
                "DROPOUT     = 0.1\n",
                "LR          = 1e-4 # Transformers often need lower learning rates than LSTMs\n",
                "BATCH_SIZE  = 64\n",
                "EPOCHS      = 100\n",
                "PATIENCE    = 12\n",
                "SEED        = 42\n",
                "\n",
                "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"✓ Using Device: {DEVICE}\")\n",
                "\n",
                "# Reproducibility\n",
                "np.random.seed(SEED)\n",
                "torch.manual_seed(SEED)\n",
                "if torch.cuda.is_available():\n",
                "    torch.cuda.manual_seed_all(SEED)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "variants_md",
            "metadata": {},
            "source": [
                "## 1. Load Data & Define Variants\n",
                "We use the same feature mapping as Stage 2 to keep our experiments consistent."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "variants_code",
            "metadata": {},
            "outputs": [],
            "source": [
                "VARIANTS = {\n",
                "    \"Transformer-Raw\": [\"log_return\"],\n",
                "    \"Transformer-OHLCV\": [\n",
                "        \"log_return\", \"oc_return\", \"hl_range\", \n",
                "        \"close_pos\", \"log_vol\", \"vol_change\"\n",
                "    ],\n",
                "    \"Transformer-Feat\": [\n",
                "        \"log_return\", \"roll_vol\", \"range_norm\", \n",
                "        \"vol_zscore\", \"mkt_return\"\n",
                "    ]\n",
                "}\n",
                "\n",
                "panel = pd.read_parquet(DATA_PATH)\n",
                "print(f\"✓ Loaded Master Panel: {panel.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "architecture_md",
            "metadata": {},
            "source": [
                "## 2. Transformer Architecture\n",
                "Transformers are \"order-agnostic\" by default, so we must add **Positional Encoding** to tell the model which day is $t-1$ and which is $t-20$."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "architecture_code",
            "metadata": {},
            "outputs": [],
            "source": [
                "class PositionalEncoding(nn.Module):\n",
                "    def __init__(self, d_model, max_len=500):\n",
                "        super().__init__()\n",
                "        pe = torch.zeros(max_len, d_model)\n",
                "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
                "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
                "        pe[:, 0::2] = torch.sin(position * div_term)\n",
                "        pe[:, 1::2] = torch.cos(position * div_term)\n",
                "        self.register_buffer('pe', pe)\n",
                "\n",
                "    def forward(self, x):\n",
                "        # x shape: (Batch, Seq, D_Model)\n",
                "        return x + self.pe[:x.size(1), :]\n",
                "\n",
                "class TransformerForecaster(nn.Module):\n",
                "    def __init__(self, input_dim, embed_dim, num_heads, num_layers, dropout):\n",
                "        super().__init__()\n",
                "        self.encoder = nn.Linear(input_dim, embed_dim)\n",
                "        self.pos_encoder = PositionalEncoding(embed_dim)\n",
                "        \n",
                "        encoder_layer = nn.TransformerEncoderLayer(\n",
                "            d_model=embed_dim, \n",
                "            nhead=num_heads, \n",
                "            dim_feedforward=embed_dim*4, \n",
                "            dropout=dropout, \n",
                "            batch_first=True\n",
                "        )\n",
                "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
                "        \n",
                "        self.head = nn.Sequential(\n",
                "            nn.Linear(embed_dim, 32),\n",
                "            nn.ReLU(),\n",
                "            nn.Linear(32, 1)\n",
                "        )\n",
                "\n",
                "    def forward(self, x):\n",
                "        x = self.encoder(x)          # (B, L, E)\n",
                "        x = self.pos_encoder(x)      # (B, L, E)\n",
                "        x = self.transformer(x)       # (B, L, E)\n",
                "        \n",
                "        # Global Average Pooling over the sequence\n",
                "        x = x.mean(dim=1)            # (B, E)\n",
                "        return self.head(x).squeeze(-1)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "helpers_md",
            "metadata": {},
            "source": [
                "## 3. Training & Evaluation Helpers\n",
                "Identical to Stage 2 for a fair comparison."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "helpers_code",
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_sequences(features, target, lookback):\n",
                "    X, y = [], []\n",
                "    for i in range(len(features) - lookback):\n",
                "        X.append(features[i : i + lookback])\n",
                "        y.append(target[i + lookback])\n",
                "    return np.array(X), np.array(y)\n",
                "\n",
                "def compute_metrics(y_true, y_pred):\n",
                "    mae = mean_absolute_error(y_true, y_pred)\n",
                "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
                "    dir_acc = np.mean(np.sign(y_true) == np.sign(y_pred))\n",
                "    return mae, rmse, dir_acc\n",
                "\n",
                "def train_model(model, train_dl, val_dl, epochs, lr, patience, device):\n",
                "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
                "    criterion = nn.MSELoss()\n",
                "    best_val_loss = float('inf')\n",
                "    wait = 0\n",
                "    best_state = None\n",
                "    \n",
                "    for epoch in range(epochs):\n",
                "        model.train()\n",
                "        for xb, yb in train_dl:\n",
                "            xb, yb = xb.to(device), yb.to(device)\n",
                "            optimizer.zero_grad()\n",
                "            loss = criterion(model(xb), yb)\n",
                "            loss.backward()\n",
                "            optimizer.step()\n",
                "            \n",
                "        model.eval()\n",
                "        v_loss = 0\n",
                "        with torch.no_grad():\n",
                "            for xb, yb in val_dl:\n",
                "                xb, yb = xb.to(device), yb.to(device)\n",
                "                v_loss += criterion(model(xb), yb).item() * len(xb)\n",
                "        v_loss /= len(val_dl.dataset)\n",
                "        \n",
                "        if v_loss < best_val_loss:\n",
                "            best_val_loss = v_loss\n",
                "            best_state = model.state_dict()\n",
                "            wait = 0\n",
                "        else:\n",
                "            wait += 1\n",
                "            if wait >= patience: break\n",
                "            \n",
                "    model.load_state_dict(best_state)\n",
                "    return model"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "pipeline_md",
            "metadata": {},
            "source": [
                "## 4. Execution Pipeline\n",
                "We loop through each variant and each ticker."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "pipeline_code",
            "metadata": {},
            "outputs": [],
            "source": [
                "tickers = [t for t in panel.index.get_level_values(\"ticker\").unique() if t != \"SPY\"]\n",
                "stage3_results = []\n",
                "\n",
                "for var_name, feat_cols in VARIANTS.items():\n",
                "    print(f\"\\n{'='*40}\\nEvaluating Variant: {var_name}\\n{'='*40}\")\n",
                "    \n",
                "    for ticker in tqdm(tickers, desc=f\"{var_name} Training\"):\n",
                "        tk_data = panel.xs(ticker, level=\"ticker\")\n",
                "        feats = tk_data[feat_cols].values\n",
                "        target = tk_data[\"log_return\"].values\n",
                "        \n",
                "        X_all, y_all = create_sequences(feats, target, LOOKBACK)\n",
                "        \n",
                "        n_seq = len(X_all)\n",
                "        split1 = int(n_seq * TRAIN_FRAC)\n",
                "        split2 = int(n_seq * (TRAIN_FRAC + VAL_FRAC))\n",
                "        \n",
                "        X_tr, y_tr = X_all[:split1], y_all[:split1]\n",
                "        X_va, y_va = X_all[split1:split2], y_all[split1:split2]\n",
                "        X_te, y_te = X_all[split2:], y_all[split2:]\n",
                "        \n",
                "        # Scale\n",
                "        scaler = StandardScaler()\n",
                "        scaler.fit(X_tr.reshape(-1, len(feat_cols)))\n",
                "        X_tr = scaler.transform(X_tr.reshape(-1, len(feat_cols))).reshape(X_tr.shape)\n",
                "        X_va = scaler.transform(X_va.reshape(-1, len(feat_cols))).reshape(X_va.shape)\n",
                "        X_te = scaler.transform(X_te.reshape(-1, len(feat_cols))).reshape(X_te.shape)\n",
                "        \n",
                "        tr_dl = DataLoader(TensorDataset(torch.FloatTensor(X_tr), torch.FloatTensor(y_tr)), batch_size=BATCH_SIZE, shuffle=True)\n",
                "        va_dl = DataLoader(TensorDataset(torch.FloatTensor(X_va), torch.FloatTensor(y_va)), batch_size=BATCH_SIZE, shuffle=False)\n",
                "        \n",
                "        model = TransformerForecaster(len(feat_cols), EMBED_DIM, NUM_HEADS, NUM_LAYERS, DROPOUT).to(DEVICE)\n",
                "        model = train_model(model, tr_dl, va_dl, EPOCHS, LR, PATIENCE, DEVICE)\n",
                "        \n",
                "        model.eval()\n",
                "        with torch.no_grad():\n",
                "            y_pred = model(torch.FloatTensor(X_te).to(DEVICE)).cpu().numpy()\n",
                "            \n",
                "        mae, rmse, dacc = compute_metrics(y_te, y_pred)\n",
                "        stage3_results.append({\"Ticker\": ticker, \"Model\": var_name, \"MAE\": mae, \"RMSE\": rmse, \"DirAcc\": dacc})\n",
                "\n",
                "res_df = pd.DataFrame(stage3_results)\n",
                "res_df.to_csv(f\"{RESULTS_DIR}/stage3_results.csv\", index=False)\n",
                "print(f\"\\n✓ Stage 3 Results saved to {RESULTS_DIR}/stage3_results.csv\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "viz_md",
            "metadata": {},
            "source": [
                "## 5. Visual Summary\n",
                "Comparison of average accuracy and error rates across the three variants."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "viz_code",
            "metadata": {},
            "outputs": [],
            "source": [
                "summary = res_df.groupby(\"Model\")[[\"MAE\", \"RMSE\", \"DirAcc\"]].mean().sort_values(\"RMSE\")\n",
                "display(summary.style.background_gradient(cmap=\"Oranges\"))\n",
                "\n",
                "plt.figure(figsize=(12, 6))\n",
                "sns.boxplot(data=res_df, x=\"Model\", y=\"DirAcc\", palette=\"Set2\")\n",
                "plt.axhline(0.5, color='red', linestyle='--')\n",
                "plt.title(\"Directional Accuracy Distribution by Transformer Variant\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "detailed_explanation",
            "metadata": {},
            "source": [
                "# Stage 3: Transformer with Attention — Detailed Summary\n",
                "\n",
                "## What This Stage Does\n",
                "\n",
                "Stage 3 evolves our deep learning approach by introducing the **Transformer** architecture. Unlike the LSTMs used in Stage 2, which process sequences step-by-step, the Transformer uses **Self-Attention** to process the entire lookback window in parallel.\n",
                "\n",
                "This answers a key architectural question:\n",
                "\n",
                "> **Does context-aware attention capture market dynamics better than sequential recurrent processing?**\n",
                "\n",
                "---\n",
                "\n",
                "## Key Differences from Stage 2 (LSTM)\n",
                "\n",
                "| Feature | Stage 2 LSTM | Stage 3 Transformer |\n",
                "|---------|--------------|---------------------|\n",
                "| **Temporal Processing** | Sequential (step-by-step) | Parallel (Self-Attention) |\n",
                "| **Memory** | Recurrent hidden state (prone to forgetting) | Global window access (attends to all days) |\n",
                "| **Positional Info** | Implicit in processing order | Explicit via Positional Encodings |\n",
                "| **Training Speed** | Slower (sequential overhead) | Faster (parallelizable operations) |\n",
                "\n",
                "--- \n",
                "\n",
                "## The Transformer Architecture\n",
                "\n",
                "Our model is built with the following components:\n",
                "\n",
                "1. **Input Embedding**: A linear layer that projects raw features into a higher-dimensional space (`EMBED_DIM=64`).\n",
                "2. **Positional Encoding**: Since Transformers don't naturally understand time order, we use sinusoidal functions to \"tag\" each day in our 20-day window with its relative position.\n",
                "3. **Multi-Head Attention**: 4 separate attention heads that allow the model to simultaneously look for different patterns (e.g., one head for momentum, one for volatility spikes).\n",
                "4. **Global Average Pooling**: Instead of just taking the \"last\" step (like in Stage 2), we average the representations across all 20 days to capture a holistic view of the window.\n",
                "5. **MLP Regressor**: A final feed-forward network that maps the pooled features to a single next-day return prediction.\n",
                "\n",
                "--- \n",
                "\n",
                "## What to Expect\n",
                "\n",
                "### Why results might be similar to Stage 2\n",
                "The stock market is essentially a **\"Low Signal, High Noise\"** environment. While Transformers are world-class at structured tasks like language modeling, financial data is often dominated by random walks. You should expect:\n",
                "\n",
                "- **Mean Performance**: Directional Accuracy hovering between **49% and 53%**.\n",
                "- **Variant Winner**: `Transformer-Feat` typically outperforms `Raw`, proving that even attention-based models benefit from pre-calculated volatility and market context.\n",
                "- **Robustness**: Transformers are often more resistant to the \"gradient vanishing\" issues that can sometimes affect LSTMs on very long sequences.\n",
                "\n",
                "--- \n",
                "\n",
                "## Connection to Project Timeline\n",
                "We have now tested both **Sequential (LSTM)** and **Global (Transformer)** deep learning. \n",
                "Next, in **Stage 4**, we pivot from \"Point Estimates\" (predicting one number) to **Probabilistic Forecasting**, where we predict the *range* and *uncertainty* of future returns."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}